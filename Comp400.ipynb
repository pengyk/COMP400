{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comp400.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LFxSvdAwYLe1"
      ],
      "toc_visible": true,
      "mount_file_id": "1dr00-7GGgXL5O1Cwz6F_1qWaW_U2nXSG",
      "authorship_tag": "ABX9TyOhWR5IYtX4URiJPMsidewk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pengyk/COMP400/blob/master/Comp400.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh6pHV-3MhBJ"
      },
      "source": [
        "pip install rasterio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKLOx1v6savQ"
      },
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import re\n",
        "import rasterio\n",
        "from rasterio import Affine, features\n",
        "from shapely.geometry import mapping, shape, Polygon, Point\n",
        "from shapely.ops import cascaded_union\n",
        "from math import floor, ceil, sqrt\n",
        "from google.colab import files\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth, DBSCAN, KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from itertools import cycle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import scipy.stats as st\n",
        "from sklearn import metrics\n",
        "\n",
        "# global paths\n",
        "base_path = \"drive/My Drive/yunkai/\"\n",
        "data_path = base_path + 'data'\n",
        "solutions_path = base_path + \"solutions.csv\"\n",
        "scaled_solutions_path = base_path + \"scaled_solutions.csv\"\n",
        "output_path = base_path + \"output\"\n",
        "path_to_consensus_json = base_path + \"consensus.json\"\n",
        "clrs_path = base_path + \"clrs\"\n",
        "consensus_path = base_path + \"consensus\"\n",
        "average_polygon_path = base_path + \"average_polygon\"\n",
        "\n",
        "np_load_old = np.load\n",
        "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WShHcLFARnzt"
      },
      "source": [
        "## Script for scaling up to original size the coordinates for the games played by users"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5yU0qFEElRz"
      },
      "source": [
        "def scale_solutions():\n",
        "\n",
        "  df = pd.read_csv(solutions_path, names=['filepath','score','result','minmax','player','created_at'], header=0, sep=\"','\", dtype={\"score\" :np.float64})\n",
        "  df[\"filepath\"] = df[\"filepath\"].str[1:]\n",
        "  df[\"created_at\"] = df[\"created_at\"].str[:-1]\n",
        "  df[\"result\"] = df[\"result\"].apply(json.loads)\n",
        "  df[\"minmax\"] = df[\"minmax\"].apply(json.loads)\n",
        "  df[\"scaled_polygons\"] = \"\"\n",
        "  df[\"mins\"] = \"\"\n",
        "  df[\"maxs\"] = \"\"\n",
        "  \n",
        "  for index, row in df.iterrows():\n",
        "    scaled_polygons = []\n",
        "    polygons = np.array(df['result'][index]['polygons'])\n",
        "    df['mins'][index] = df['minmax'][index][:2]\n",
        "    mins = np.array(df['minmax'][index][:2])\n",
        "    df['maxs'][index] = df['minmax'][index][2:]\n",
        "    maxs = np.array(df['minmax'][index][2:])\n",
        "    for polygon in polygons:\n",
        "      output = np.empty((0,2), type)\n",
        "      for vertex in polygon['vertices']:\n",
        "        vertex = mins + (vertex * (maxs-mins))\n",
        "        if len(output) == 0:\n",
        "            output = vertex\n",
        "        else:\n",
        "          output = np.vstack((output, vertex))\n",
        "      scaled_polygons.append(output.tolist())\n",
        "    df[\"scaled_polygons\"][index] = scaled_polygons\n",
        "  df.to_csv(scaled_solutions_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRunqfKRY_fZ"
      },
      "source": [
        "# Consensus polygon generation based on K-Means with Average Silhouette\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxeFlpkpR32k"
      },
      "source": [
        "def random_distribution_consensus_generator(max_number_clusters = 5, x_shrink = 0.4, y_shrink = 0.4):\n",
        "  \n",
        "  # The generation will use all the available datasets that we have.\n",
        "  # It will traverse all the folders and all the files present in the clrs folder\n",
        "  # in order to obtain all the datasets.\n",
        "  folders = listdir(clrs_path)\n",
        "  for folder in folders:\n",
        "    all_files = listdir(clrs_path + '/' + folder)\n",
        "    for file_name in all_files:\n",
        "      file_name_without_csv = re.sub('\\.csv', '', file_name)\n",
        "      full_file_path = folder + '/' + file_name_without_csv\n",
        "\n",
        "      # This function fetches all games played on a specified dataset\n",
        "      polygons = random_plot_get_all_games(full_file_path)\n",
        "\n",
        "      # obtain the max and min coordinate to scale the value down to a 1x1 square\n",
        "      minX = 2147483647\n",
        "      minY = 2147483647\n",
        "      maxX = -2147483647\n",
        "      maxY = -2147483647\n",
        "      for polygon in polygons:\n",
        "        for coordinates in polygon:\n",
        "          minX = min(minX, coordinates[0])\n",
        "          maxX = max(maxX, coordinates[0])\n",
        "          minY = min(minY, coordinates[1])\n",
        "          maxY = max(maxY, coordinates[1])\n",
        "\n",
        "      # scale the coordinates between the range of [0,1]\n",
        "      scaled_polygons = []\n",
        "      for polygon in polygons:\n",
        "        temp_list = []\n",
        "        for coordinates in polygon:\n",
        "          x_val = (coordinates[0] - minX)/ (maxX - minX)\n",
        "          y_val = (coordinates[1] - minY)/ (maxY - minY)\n",
        "          temp_list.append(tuple([x_val, y_val]))\n",
        "        scaled_polygons.append(temp_list)\n",
        "\n",
        "      # If there is less than 30 games played, do not generate the consensus polygon as it is too small sample size\n",
        "      if len(polygons) < 30 :\n",
        "        continue\n",
        "\n",
        "      polygons = scaled_polygons\n",
        "\n",
        "      # Get random points per polygon in order to later plot density plot\n",
        "      scatter_plot = []\n",
        "\n",
        "      # shrink down size of each polygon to get a better separation\n",
        "      for polygon in polygons:\n",
        "        xs, ys = zip(*polygon)\n",
        "        xs = np.array(xs)\n",
        "        ys = np.array(ys)\n",
        "        x_center = 0.5 * min(xs)  + 0.5 * max(xs)\n",
        "        y_center = 0.5 * min(ys)  + 0.5 * max(ys)\n",
        "        new_xs = [(i - x_center) * (1 - x_shrink) + x_center for i in xs]\n",
        "        new_ys = [(i - y_center) * (1 - y_shrink) + y_center for i in ys]\n",
        "        # create list of new coordinates\n",
        "        polygon = zip(new_xs, new_ys)\n",
        "        poly = Polygon(polygon)\n",
        "        polygonArea = PolyArea(xs,ys)\n",
        "        for i in range(int(100 * (polygonArea))):\n",
        "          point_in_poly = random_point_in_polygon(poly)\n",
        "          scatter_plot.append([point_in_poly.x, point_in_poly.y])\n",
        "        \n",
        "      # Store it in a numpy array so that it is easier to fetch all x coordinates and all y coordinates later\n",
        "      X = np.array(scatter_plot)\n",
        "\n",
        "      # Try to get the highest average silhouette and keep track of the number of clusters that gives it\n",
        "      # Start iteration with 2 clusters and go up until the max_number_clusters\n",
        "      max_average = 0\n",
        "      best_cluster_number = 0\n",
        "      n_clusters = 2\n",
        "\n",
        "      while n_clusters <= max_number_clusters:\n",
        "        # calculate the highest Average Silhouette score\n",
        "        clusterer = KMeans(n_clusters=n_clusters)\n",
        "        cluster_labels = clusterer.fit_predict(X)\n",
        "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "        print(\"For n_clusters =\", n_clusters,\n",
        "              \"The average silhouette_score is :\", silhouette_avg)\n",
        "        if max_average < silhouette_avg:\n",
        "          max_average = silhouette_avg\n",
        "          best_cluster_number = n_clusters\n",
        "        n_clusters += 1\n",
        "\n",
        "      # Now that we have the number of clusters that generates the highest\n",
        "      # average silhouette score, we can plot the graphs.\n",
        "      n_clusters = best_cluster_number\n",
        "\n",
        "      # Clear graph just to be sure.\n",
        "      plt.cla()\n",
        "      plt.close()\n",
        "      plt.clf()\n",
        "      fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "      fig.set_size_inches(18, 7)\n",
        "\n",
        "      # The silhouette plot that ranges from -0.1 to 1.\n",
        "      ax1.set_xlim([-0.1, 1])\n",
        "      # Add a little space between average silhouette graphs.\n",
        "      ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "      # Initialize the clusterer with n_clusters value\n",
        "      clusterer = KMeans(n_clusters=n_clusters)\n",
        "      cluster_labels = clusterer.fit_predict(X)\n",
        "      # The silhouette_score gives the average value for all the samples.\n",
        "      # This gives a perspective into the density and separation of the formed\n",
        "      # clusters\n",
        "      silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "      print(\"For n_clusters =\", n_clusters,\n",
        "            \"The best average silhouette_score is :\", silhouette_avg)\n",
        "      # Compute the silhouette scores for each sample\n",
        "      sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "      y_lower = 10\n",
        "      for i in range(n_clusters):\n",
        "        # Separate points based on cluster\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "      ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "      ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "      ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "      # The vertical line for average silhouette score of all the values\n",
        "      ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "      ax1.set_yticks([])  # Clear the labels\n",
        "      ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "      # Plot showing the clusters.\n",
        "      colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "      ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,c=colors, edgecolor='k')\n",
        "      plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                    \"with n_clusters = %d\" % n_clusters),\n",
        "                  fontsize=14, fontweight='bold')\n",
        "      plt.show()\n",
        "\n",
        "      # Separate the data into different clusters, since there are varying densities,\n",
        "      # it is more precise if we can computer the contour for each cluster/polygon\n",
        "\n",
        "      # clusters is the dictionnary that contains the mapping between each cluster and its points\n",
        "      clusters = dict()\n",
        "\n",
        "      for i in range(len(colors)):\n",
        "        # We use the color as the key\n",
        "        checksum = np.array_str(colors[i])\n",
        "        if not checksum in clusters:\n",
        "          clusters[checksum] = []\n",
        "        clusters[checksum].append(X[i].tolist())\n",
        "\n",
        "      # exterior bounds will save all the outer contours for all the polygons in the plot\n",
        "      exterior_bounds = []\n",
        "\n",
        "      for cluster in clusters:\n",
        "        plt.cla()\n",
        "        plt.clf()\n",
        "        coordinates = np.array(clusters[cluster])\n",
        "        x = coordinates[:, 0]\n",
        "        y = coordinates[:, 1]\n",
        "\n",
        "        # Define the borders\n",
        "        values = np.vstack([x, y])\n",
        "        kernel = st.gaussian_kde(values)\n",
        "\n",
        "        deltaX = (max(x) - min(x))/10\n",
        "        deltaY = (max(y) - min(y))/10\n",
        "        xmin = min(x) - deltaX\n",
        "        xmax = max(x) + deltaX\n",
        "        ymin = min(y) - deltaY\n",
        "        ymax = max(y) + deltaY\n",
        "\n",
        "        # Create meshgrid\n",
        "        xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j] \n",
        "\n",
        "        positions = np.vstack([xx.ravel(), yy.ravel()])\n",
        "        values = np.vstack([x, y])\n",
        "        f = np.reshape(kernel(positions).T, xx.shape)\n",
        "        fig = plt.figure(figsize=(8,8))\n",
        "        ax = fig.gca()\n",
        "        ax.set_xlim(xmin, xmax)\n",
        "        ax.set_ylim(ymin, ymax)\n",
        "        cfset = ax.contourf(xx, yy, f, cmap='coolwarm')\n",
        "        ax.imshow(np.rot90(f), cmap='coolwarm', extent=[xmin, xmax, ymin, ymax])\n",
        "        cset = ax.contour(xx, yy, f, colors='k')\n",
        "        ax.clabel(cset, inline=1, fontsize=10)\n",
        "        ax.set_xlabel('X')\n",
        "        ax.set_ylabel('Y')\n",
        "        plt.title('2D Gaussian Kernel density estimation')\n",
        "        plt.figure(figsize=(8,8))\n",
        "        for clust in range(len(cset.allsegs)):\n",
        "          largest_segment = []\n",
        "          for lev, seg in enumerate(cset.allsegs[clust]):\n",
        "            # only the most outer level matters\n",
        "            if clust == 1:\n",
        "              # for a cluster, get the largest of the two\n",
        "              if len(largest_segment) == 0:\n",
        "                largest_segment = seg\n",
        "              else:\n",
        "                largest_xs, largest_ys = zip(*largest_segment)\n",
        "                largest_xs = np.array(largest_xs)\n",
        "                largest_ys = np.array(largest_ys)\n",
        "\n",
        "                seg_xs, seg_ys = zip(*seg)\n",
        "                seg_xs = np.array(seg_xs)\n",
        "                seg_ys = np.array(seg_ys)\n",
        "\n",
        "                if PolyArea(largest_xs, largest_ys) < PolyArea(seg_xs, seg_ys):\n",
        "                  largest_segment = seg\n",
        "              # save it to the array\n",
        "            plt.plot(seg[:,0], seg[:,1], '.-', label=f'Cluster{clust}, level{lev}')\n",
        "          if clust == 1:\n",
        "            exterior_bounds.append(largest_segment)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "      # rescale back to original size\n",
        "      original_sized_bound = []\n",
        "      for i in range(len(exterior_bounds)):\n",
        "        original_sized_bound.append([])\n",
        "        polygon = exterior_bounds[i]\n",
        "        for j in range(len(polygon)):\n",
        "          coordinates = polygon[j]\n",
        "          original_sized_bound[i].append([])\n",
        "          original_sized_bound[i][j].append(coordinates[0] * (maxX - minX) + minX)\n",
        "          original_sized_bound[i][j].append(coordinates[1] * (maxY - minY) + minY)\n",
        "      \n",
        "      np.save(average_polygon_path + '/' + folder + '~' + file_name_without_csv + '~consensus', original_sized_bound)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8TGAF0gt-I9"
      },
      "source": [
        "# Polygon area calculator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytuF6ApNqBJZ"
      },
      "source": [
        "def PolyArea(x,y):\n",
        "  return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm-t3oIHA9Ru"
      },
      "source": [
        "# Calculate max area of all the polygons for a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU3fLjpsA81r"
      },
      "source": [
        "def getTotalArea(polygons):\n",
        "  minX = 2147483647\n",
        "  minY = 2147483647\n",
        "  maxX = -2147483647\n",
        "  maxY = -2147483647\n",
        "  for polygon in polygons:\n",
        "    for coordinates in polygon:\n",
        "      minX = min(minX, coordinates[0])\n",
        "      maxX = max(maxX, coordinates[0])\n",
        "      minY = min(minY, coordinates[1])\n",
        "      maxY = max(maxY, coordinates[1])\n",
        "  return (maxX - minX) * (maxY - minY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74GUYiSVZ63B"
      },
      "source": [
        "# Function to randomly fill each polygon with points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSbG1Quc4zOk"
      },
      "source": [
        "def random_point_in_polygon(poly):\n",
        "  minx, miny, maxx, maxy = poly.bounds\n",
        "  while True:\n",
        "    p = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
        "    if poly.contains(p):\n",
        "      return p "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNqQkrgyZ_lV"
      },
      "source": [
        "# Get all games played for each graph/plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cO4oUvBrQbZ"
      },
      "source": [
        "def random_plot_get_all_games(full_file_path):\n",
        "  print(full_file_path)\n",
        "  full_file_path_csv = full_file_path + '.csv'\n",
        "  path = \"drive/My Drive/yunkai/solutions.csv\"\n",
        "\n",
        "  df = pd.read_csv(scaled_solutions_path)\n",
        "  \n",
        "  # get all games of this dataset from all players\n",
        "  output = []\n",
        "  number_of_games = 0;\n",
        "  for index, row in df.iterrows():\n",
        "    if row['filepath'] == full_file_path_csv:\n",
        "      number_of_games += 1\n",
        "      read_json  = json.loads(row['scaled_polygons'])\n",
        "      for coord in read_json:\n",
        "        output.append(coord)\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBfQe_UBpX7c"
      },
      "source": [
        "# Test to visualize consensus polygons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUR02MBTmPy0"
      },
      "source": [
        "def double_check_consensus():\n",
        "  # np.load.__defaults__=(None, True, True, 'ASCII')\n",
        "  all_files = listdir(average_polygon_path)\n",
        "  for file_name in all_files:\n",
        "    print(average_polygon_path + '/' + file_name)\n",
        "    X = np.load(average_polygon_path + '/' + file_name)\n",
        "    plt.clf()\n",
        "    plt.cla()\n",
        "    for x in X:\n",
        "      x = np.array(x)\n",
        "      plt.plot(x[:,0], x[:,1])\n",
        "    plt.title(\"Consensus generation of \" + str(file_name))\n",
        "    plt.show()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPkSgYeislpc"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw2ocOtF4jB9"
      },
      "source": [
        "scale_solutions()\n",
        "random_distribution_consensus_generator()\n",
        "double_check_consensus()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFxSvdAwYLe1"
      },
      "source": [
        "# Archived code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uziz9K4VDCO"
      },
      "source": [
        "### Check if it necessary to remove one layer of parentheses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMJ_n_v9SXBb"
      },
      "source": [
        "def remove_layer_of_parenthesis(list_of_tuples):\n",
        "  if len(list(list(list_of_tuples)[0])) == 2:\n",
        "    return False\n",
        "  else :\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17R4eRvdgvKs"
      },
      "source": [
        "## Code for plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmFFHdEXgurw"
      },
      "source": [
        "def plotGraph(folder, fileName, outputPath):\n",
        "  pathToData = \"drive/My Drive/yunkai/data/\" + folder + '/' + fileName + \".npy\"\n",
        "  pathToCluster = \"drive/My Drive/yunkai/clrs/\" + folder + '/' + fileName\n",
        "  pathToSolutions = \"drive/My Drive/yunkai/solutions.csv\"\n",
        "  \n",
        "  file = folder + '/' + fileName\n",
        "\n",
        "  #get minmax but not really necessary\n",
        "  df = pd.read_csv(pathToSolutions, names=['filepath','score','result','minmax','player','created_at'], header=0, sep=\"','\", dtype={\"score\" :np.float64})\n",
        "  \n",
        "  df[\"filepath\"] = df[\"filepath\"].str[1:]\n",
        "  df[\"created_at\"] = df[\"created_at\"].str[:-1]\n",
        "  df[\"result\"] = df[\"result\"].apply(json.loads)\n",
        "  df[\"minmax\"] = df[\"minmax\"].apply(json.loads)\n",
        "\n",
        "  mins = df.loc[df['filepath'] == file]['minmax'].iloc[0][:2]\n",
        "  maxs = df.loc[df['filepath'] == file]['minmax'].iloc[0][2:]\n",
        "\n",
        "  allData = np.load(pathToData)\n",
        "  # x = mins[0] + (allData[:,0] * (maxs[0] - mins[0]))\n",
        "  # y = mins[1] + (allData[:,1] * (maxs[1] - mins[1]))\n",
        "  x = allData[:,0]\n",
        "  y = allData[:,1]\n",
        "\n",
        "  #get cluster points\n",
        "  myColors = dict()\n",
        "  df = pd.read_csv(pathToCluster)\n",
        "  color = 0  \n",
        "  legend = [''] * len(df.columns)\n",
        "  # print(legend)\n",
        "  #mark color for each column\n",
        "  for column in df.columns:\n",
        "    myColors[column] = color\n",
        "    legend[color] = column + ':' + str(color)\n",
        "    color += 1\n",
        "\n",
        "  #get the colormap\n",
        "  colorMap = [None] * len(x)\n",
        "  for index, row in df.iterrows():\n",
        "    for i in range(len(row)):\n",
        "      if(row[i] == True):\n",
        "        colorMap[index] = i\n",
        "        break\n",
        "  \n",
        "  # print(\"my colors\")\n",
        "  # print(colorMap)\n",
        "  \n",
        "  plt.scatter(x, y, c = colorMap, alpha=0.2, label=legend)\n",
        "  plt.colorbar()\n",
        "  plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left',\n",
        "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
        "\n",
        "  # plt.show()\n",
        "\n",
        "  fileNameWithoutCSV = re.sub('\\.csv', '', fileName)\n",
        "  print(fileNameWithoutCSV)\n",
        "  plt.savefig(outputPath + '/' + folder + '/' + fileNameWithoutCSV + '.png')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Cd2mgFsIwP"
      },
      "source": [
        "## Script to plot graph for all datasets (optional)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZAfhQH1sIVt"
      },
      "source": [
        "def plotAllGraphs():\n",
        "  clrsPath = 'drive/My Drive/yunkai/clrs'\n",
        "  outputPath = 'drive/My Drive/yunkai/output'\n",
        "  folders = listdir(clrsPath)\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(outputPath + '/' + folder):\n",
        "      os.makedirs(outputPath + '/' + folder)\n",
        "    files = listdir(clrsPath + '/' + folder)\n",
        "    for fileName in files:\n",
        "      full_file_path = folder + '/' + fileName\n",
        "      print(full_file_path)\n",
        "      plotGraph(folder, fileName, outputPath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v71lZTQR7AzL"
      },
      "source": [
        "## Get all games for the same dataset and add them in a shapefile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxLK1irdGj7z"
      },
      "source": [
        "def get_all_games_of_dataset(full_file_path):\n",
        "  print(full_file_path)\n",
        "  full_file_path_csv = full_file_path + '.csv'\n",
        "\n",
        "  df = pd.read_csv(scaled_solutions_path)\n",
        "  df[\"filepath\"] = df[\"filepath\"].str[1:]\n",
        "  df[\"created_at\"] = df[\"created_at\"].str[:-1]\n",
        "  df[\"result\"] = df[\"result\"].apply(json.loads)\n",
        "  df[\"minmax\"] = df[\"minmax\"].apply(json.loads)\n",
        "\n",
        "  # get all games of this dataset from all players\n",
        "  output = []\n",
        "  for index, row in df.iterrows():\n",
        "    if row['filepath'] == full_file_path_csv:\n",
        "      number_of_games += 1\n",
        "      number_of_polygons += len(row['result']['polygons'])\n",
        "      for polygon in row['result']['polygons']:\n",
        "        coord = polygon['vertices']\n",
        "        coord = np.array(coord)\n",
        "        # times 10000 to scale up for later\n",
        "        coord = [i*10000 for i in coord]\n",
        "        # transform it into a shapefile format\n",
        "        coord=[tuple(i) for i in coord]\n",
        "        output.append({\n",
        "            'type': 'Polygon',\n",
        "            'coordinates': [coord]\n",
        "        })\n",
        "  print(\"numer of polygons is of length\" + str(number_of_polygons))\n",
        "  if len(output) == 0:\n",
        "    return (output, 0)\n",
        "  average = round(number_of_polygons/number_of_games)\n",
        "  # in case if the average is 1, assume that it is impossible and round it up to 2\n",
        "  if average == 1:\n",
        "    average += 1\n",
        "  print(\"average number of polygons is: \" + str(number_of_polygons/number_of_games) + \" rounded: \" + str(average))\n",
        "  # return all the polygons and the average number of polygons found by each player\n",
        "  return (output, average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6_pdtuC7RnT"
      },
      "source": [
        "## Rasterization method to generate consensus\n",
        "## funtion that generates all the consensus polygons for all datasets, does not work as well compared to simple matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4orDu270b2P"
      },
      "source": [
        "def get_all_data():\n",
        "  output_path = \"drive/My Drive/yunkai/output\"\n",
        "  path_to_consensus_json = \"drive/My Drive/yunkai/consensus.json\"\n",
        "  folders = listdir(clrs_path)\n",
        "  for folder in folders:\n",
        "    # create consensus folder for each data group\n",
        "    if not os.path.exists(consensus_path + '/' + folder):\n",
        "      os.makedirs(consensus_path + '/' + folder)\n",
        "    all_files = listdir(clrs_path + '/' + folder)\n",
        "    for file_name in all_files:\n",
        "      file_name_without_csv = re.sub('\\.csv', '', file_name)\n",
        "      full_file_path = folder + '/' + file_name_without_csv\n",
        "      # create consensus folder for each file_name too\n",
        "      if not os.path.exists(consensus_path + '/' + folder + '/' + file_name_without_csv):\n",
        "        os.makedirs(consensus_path + '/' + folder + '/' + file_name_without_csv)\n",
        "      # this function fetches all games played on a specified dataset\n",
        "      (shapes,average) = get_all_games_of_dataset(full_file_path)\n",
        "      # if there isn't any shape returned from the dataset, then the game has not been played yet\n",
        "      if len(shapes) == 0 :\n",
        "        print(\"this game hasn't been played yet\")\n",
        "        continue  \n",
        "      max_shape = cascaded_union([shape(s) for s in shapes])\n",
        "      minx, miny, maxx, maxy = max_shape.bounds\n",
        "      # change it to 0.001 for precision\n",
        "      dx = dy = 1.0  # grid resolution\n",
        "      lenx = dx * (ceil(maxx / dx) - floor(minx / dx))\n",
        "      leny = dy * (ceil(maxy / dy) - floor(miny / dy))\n",
        "      assert lenx % dx == 0.0\n",
        "      assert leny % dy == 0.0\n",
        "      nx = int(lenx / dx)\n",
        "      ny = int(leny / dy)\n",
        "      gt = Affine(dx, 0.0, dx * floor(minx / dx),0.0, -dy, dy * ceil(maxy / dy))\n",
        "      pa = np.zeros((ny, nx), 'd')\n",
        "      count = 0\n",
        "      for s in shapes:\n",
        "        try:\n",
        "          r = features.rasterize([s], (ny, nx), transform=gt)\n",
        "          pa[r > 0] += 1\n",
        "          count += 1\n",
        "        except:\n",
        "          pass\n",
        "      pa /= count  # normalise values\n",
        "      plt.imshow(pa)\n",
        "      plt.show()\n",
        "      spa, sgt = gaussian_blur(pa, gt, 100)\n",
        "      number_good_size_poly = 0\n",
        "      decrement = 0\n",
        "      best_threshold = 0\n",
        "      best_average_size = 0\n",
        "      while decrement <= 0.4: \n",
        "        plt.clf()\n",
        "        thresh = 0.5-decrement  # median\n",
        "        decrement += 0.05\n",
        "        print(\"threshold is now: \" + str(thresh))\n",
        "        pm = np.zeros(spa.shape, 'B')\n",
        "        pm[spa > thresh] = 1\n",
        "        poly_shapes = []\n",
        "        for sh, val in features.shapes(pm, transform=sgt):\n",
        "          if val == 1:\n",
        "              poly_shapes.append(shape(sh))\n",
        "        if not any(poly_shapes):\n",
        "          print(\"not good threshold to find shapes, decrement it\")\n",
        "          continue\n",
        "        avg_poly = cascaded_union(poly_shapes)\n",
        "        # Simplify the polygon\n",
        "        simp_poly = avg_poly.simplify(sqrt(dx**2 + dy**2))\n",
        "        simp_shape = mapping(simp_poly)\n",
        "        number_good_size_poly = 0\n",
        "        print(\"length of list of simp_shapes\" + str(len(list(simp_shape['coordinates']))))\n",
        "        # it's just one big cluster, not good\n",
        "        if (len(list(simp_shape['coordinates']))) == 1 :\n",
        "          print(\"length of 1 doesn't work, it is just a big cluster\")\n",
        "          continue\n",
        "        list_of_areas = []\n",
        "        total_size = 0\n",
        "        to_list_simp_shape = [list(i) for i in list(simp_shape['coordinates'])]\n",
        "        for list_of_tuples in to_list_simp_shape:\n",
        "          if remove_layer_of_parenthesis(list_of_tuples):\n",
        "            list_of_tuples = list(list(list_of_tuples)[0])\n",
        "          list_of_lists = [list(i) for i in list_of_tuples]\n",
        "          xs, ys = zip(*list_of_lists)\n",
        "          xs = np.array(xs)\n",
        "          ys = np.array(ys)\n",
        "          if PolyArea(xs,ys) > 1000000:\n",
        "            # print(list_of_lists)\n",
        "            list_of_areas.append(PolyArea(xs,ys))\n",
        "            total_size += PolyArea(xs,ys)\n",
        "            plt.plot(xs, ys, c=np.random.rand(3, ), alpha=0.5)\n",
        "            number_good_size_poly += 1\n",
        "        print(\"number of polygons of good size: \" + str(number_good_size_poly))\n",
        "        if number_good_size_poly == average:\n",
        "          average_size = 1\n",
        "          for area in list_of_areas:\n",
        "            average_size *= area/total_size\n",
        "          if average_size > best_average_size:\n",
        "            best_average_size = average_size\n",
        "            best_threshold = thresh\n",
        "        else :\n",
        "          print(\"this file can't find the good size\")\n",
        "      if best_threshold != 0:\n",
        "        plt.clf()\n",
        "        thresh = best_threshold\n",
        "        pm = np.zeros(spa.shape, 'B')\n",
        "        pm[spa > thresh] = 1\n",
        "        poly_shapes = []\n",
        "        for sh, val in features.shapes(pm, transform=sgt):\n",
        "          if val == 1:\n",
        "              poly_shapes.append(shape(sh))\n",
        "        avg_poly = cascaded_union(poly_shapes)\n",
        "        # Simplify the polygon\n",
        "        simp_poly = avg_poly.simplify(sqrt(dx**2 + dy**2))\n",
        "        simp_shape = mapping(simp_poly)\n",
        "        print(\"length of list of simp_shapes\" + str(len(list(simp_shape['coordinates']))))\n",
        "        # it's just one big cluster, not good\n",
        "        if (len(list(simp_shape['coordinates']))) == 1 :\n",
        "          print(\"length of 1 doesn't work\")\n",
        "          continue\n",
        "        list_of_areas = []\n",
        "        listToSave = []\n",
        "        to_list_simp_shape = [list(i) for i in list(simp_shape['coordinates'])]\n",
        "        count_poly = 0\n",
        "        for list_of_tuples in to_list_simp_shape:\n",
        "          if remove_layer_of_parenthesis(list_of_tuples):\n",
        "            list_of_tuples = list(list(list_of_tuples)[0])\n",
        "          list_of_lists = [list(i) for i in list_of_tuples]\n",
        "          xs, ys = zip(*list_of_lists)\n",
        "          xs = np.array(xs)\n",
        "          ys = np.array(ys)\n",
        "          if PolyArea(xs,ys) > 1000000:\n",
        "            # print(list_of_lists)\n",
        "            list_of_areas.append(PolyArea(xs,ys))\n",
        "            plt.plot(xs, ys, c=np.random.rand(3, ), alpha=0.5)\n",
        "            listToSave.append([])\n",
        "            listToSave[count_poly] = (list_of_lists)\n",
        "            print(\"count is \" + str(count_poly))\n",
        "            print(listToSave)\n",
        "            count_poly += 1\n",
        "        print(\"length of list to save\")\n",
        "        print(len(listToSave))\n",
        "        # df.append({\n",
        "        #     \"filepath\": folder + '/' + file_name_without_csv,\n",
        "        #     \"consensus polygons\": listToSave\n",
        "        # }, ignore_index=True)\n",
        "        print(listToSave)\n",
        "        with open(path_to_consensus_json, \"r+\") as json_file:\n",
        "          data = json.load(json_file)\n",
        "          data.append({\n",
        "            \"filepath\": folder + '/' + file_name_without_csv,\n",
        "            \"consensus polygons\": listToSave\n",
        "          })\n",
        "          print(data)\n",
        "          with open(path_to_consensus_json, 'w') as f: \n",
        "            json.dump(data, f) \n",
        "        print(\"got it\")\n",
        "        name = file_name_without_csv + \".png\"\n",
        "        plt.savefig(output_path + '/' + folder + '/' + name)\n",
        "        plt.clf()\n",
        "        \n",
        "      print(\"done for all files in this folder :)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKqPIHdIqHQJ"
      },
      "source": [
        "def get_all_games_of_dataset(full_file_path):\n",
        "  print(full_file_path)\n",
        "  full_file_path_csv = full_file_path + '.csv'\n",
        "  path = \"drive/My Drive/yunkai/solutions.csv\"\n",
        "\n",
        "  df = pd.read_csv(path, names=['filepath','score','result','minmax','player','created_at'], header=0, sep=\"','\", dtype={\"score\" :np.float64})\n",
        "  df[\"filepath\"] = df[\"filepath\"].str[1:]\n",
        "  df[\"created_at\"] = df[\"created_at\"].str[:-1]\n",
        "  df[\"result\"] = df[\"result\"].apply(json.loads)\n",
        "  df[\"minmax\"] = df[\"minmax\"].apply(json.loads)\n",
        "\n",
        "  # get all games of this dataset from all players\n",
        "  output = []\n",
        "  number_of_games = 0;\n",
        "  number_of_polygons = 0;\n",
        "  for index, row in df.iterrows():\n",
        "    if row['filepath'] == full_file_path_csv:\n",
        "      number_of_games += 1\n",
        "      number_of_polygons += len(row['result']['polygons'])\n",
        "      for polygon in row['result']['polygons']:\n",
        "        coord = polygon['vertices']\n",
        "        coord = np.array(coord)\n",
        "        # times 10000 to scale up for later\n",
        "        coord = [i*10000 for i in coord]\n",
        "        # transform it into a shapefile format\n",
        "        coord=[tuple(i) for i in coord]\n",
        "        output.append({\n",
        "            'type': 'Polygon',\n",
        "            'coordinates': [coord]\n",
        "        })\n",
        "  print(\"numer of polygons is of length\" + str(number_of_polygons))\n",
        "  if len(output) == 0:\n",
        "    return (output, 0)\n",
        "  average = round(number_of_polygons/number_of_games)\n",
        "  # in case if the average is 1, assume that it is impossible and round it up to 2\n",
        "  if average == 1:\n",
        "    average += 1\n",
        "  print(\"average number of polygons is: \" + str(number_of_polygons/number_of_games) + \" rounded: \" + str(average))\n",
        "  # return all the polygons and the average number of polygons found by each player\n",
        "  return (output, average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HELHfU3P7PW_"
      },
      "source": [
        "## Scale array to 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAtivDoo797e"
      },
      "source": [
        "def scale_down (folder, file_name) :\n",
        "  path_to_data = \"drive/My Drive/yunkai/data/\" + folder + '/' + file_name + \".csv.npy\"\n",
        "  allData = np.load(path_to_data)\n",
        "  print(allData)\n",
        "  xs, ys = zip(*allData)\n",
        "  minX = min(xs)\n",
        "  minY = min(ys)\n",
        "  maxX = max(xs)\n",
        "  maxY = max(ys)\n",
        "  # to scale between 0 and 1\n",
        "  xs = (xs - minX) / (maxX - minX)\n",
        "  ys = (ys - minY) / (maxY - minY)\n",
        "  # xs = np.asarray(xs)\n",
        "  # ys = np.asarray(ys)\n",
        "  # print(xs)\n",
        "  # print(ys)\n",
        "  scaled = np.vstack((xs, ys))\n",
        "  scaled = np.transpose(scaled)\n",
        "  print(folder + \"/\" + file_name)\n",
        "  print(scaled)\n",
        "  saved_path = \"drive/My Drive/yunkai/data/\" + folder + '/scaled_' + file_name + \".csv.npy\"\n",
        "  np.save(saved_path, scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_5SGYcCcAno"
      },
      "source": [
        "def scale_all_data():\n",
        "  path_to_data = \"drive/My Drive/yunkai/data/\"\n",
        "\n",
        "  folders = listdir(path_to_data)\n",
        "  for folder in folders:\n",
        "    all_files = listdir(path_to_data + '/' + folder)\n",
        "    for file_name in all_files:\n",
        "      file_name = re.sub('\\.npy', '', file_name)\n",
        "      file_name = re.sub('\\.csv', '', file_name)\n",
        "      scale_down(folder, file_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltj-OnSjs_1i"
      },
      "source": [
        "## Gaussian Blur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hf_Qqj_VEue"
      },
      "source": [
        "from scipy.signal import fftconvolve\n",
        "\n",
        "def gaussian_blur(in_array, gt, size):\n",
        "    \"\"\"Gaussian blur, returns tuple `(ar, gt2)` that have been expanded by `size`\"\"\"\n",
        "    # expand in_array to fit edge of kernel; constant value is zero\n",
        "    padded_array = np.pad(in_array, size, 'constant')\n",
        "    # build kernel\n",
        "    x, y = np.mgrid[-size:size + 1, -size:size + 1]\n",
        "    g = np.exp(-(x**2 / float(size) + y**2 / float(size)))\n",
        "    g = (g / g.sum()).astype(in_array.dtype)\n",
        "    # do the Gaussian blur\n",
        "    ar = fftconvolve(padded_array, g, mode='full')\n",
        "    # convolved increased size of array ('full' option); update geotransform\n",
        "    gt2 = Affine(\n",
        "        gt.a, gt.b, gt.xoff - (2 * size * gt.a),\n",
        "        gt.d, gt.e, gt.yoff - (2 * size * gt.e))\n",
        "    return ar, gt2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e9r_5DTTwmr"
      },
      "source": [
        "## Preprocess data and generate k clusters based on mean shift s.t. it represents 10 games \"played by gamers\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBN_ZxyNVikg"
      },
      "source": [
        "def generate_all_ml_polygons():\n",
        "  folders = listdir(data_path)\n",
        "  for folder in folders:\n",
        "    all_files = listdir(data_path + '/' + folder)\n",
        "    for file_name in all_files:\n",
        "      print(folder + '/' + file_name)\n",
        "      X = np.load(data_path + '/' + folder + '/' + file_name)\n",
        "      range_n_clusters = [2, 3, 4, 5]\n",
        "      max_average = 0\n",
        "      best_cluster_number = -1\n",
        "      # for run_number in range(3):\n",
        "      for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "        plt.cla()\n",
        "        plt.close()\n",
        "        plt.clf()\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "        fig.set_size_inches(18, 7)\n",
        "\n",
        "        # The 1st subplot is the silhouette plot\n",
        "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "        # lie within [-0.1, 1]\n",
        "        ax1.set_xlim([-0.1, 1])\n",
        "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "        # plots of individual clusters, to demarcate them clearly.\n",
        "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "        # Initialize the clusterer with n_clusters value and a random generator\n",
        "        # seed of 10 for reproducibility.\n",
        "        clusterer = KMeans(n_clusters=n_clusters)\n",
        "        cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "        # The silhouette_score gives the average value for all the samples.\n",
        "        # This gives a perspective into the density and separation of the formed\n",
        "        # clusters\n",
        "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "        print(\"For n_clusters =\", n_clusters,\n",
        "              \"The average silhouette_score is :\", silhouette_avg)\n",
        "        if silhouette_avg > 0.65 and max_average < silhouette_avg:\n",
        "          max_average = silhouette_avg\n",
        "          best_cluster_number = n_clusters\n",
        "      if best_cluster_number != -1:\n",
        "        n_clusters = best_cluster_number\n",
        "        plt.cla()\n",
        "        plt.close()\n",
        "        plt.clf()\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "        fig.set_size_inches(18, 7)\n",
        "\n",
        "        # The 1st subplot is the silhouette plot\n",
        "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "        # lie within [-0.1, 1]\n",
        "        ax1.set_xlim([-0.1, 1])\n",
        "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "        # plots of individual clusters, to demarcate them clearly.\n",
        "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "        # Initialize the clusterer with n_clusters value and a random generator\n",
        "        clusterer = KMeans(n_clusters=n_clusters)\n",
        "        cluster_labels = clusterer.fit_predict(X)\n",
        "        # The silhouette_score gives the average value for all the samples.\n",
        "        # This gives a perspective into the density and separation of the formed\n",
        "        # clusters\n",
        "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "        print(\"For n_clusters =\", n_clusters,\n",
        "              \"The average silhouette_score is :\", silhouette_avg)\n",
        "        # Compute the silhouette scores for each sample\n",
        "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "        y_lower = 10\n",
        "        for i in range(n_clusters):\n",
        "            # Aggregate the silhouette scores for samples belonging to\n",
        "            # cluster i, and sort them\n",
        "            ith_cluster_silhouette_values = \\\n",
        "                sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "            ith_cluster_silhouette_values.sort()\n",
        "\n",
        "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "            y_upper = y_lower + size_cluster_i\n",
        "\n",
        "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                              0, ith_cluster_silhouette_values,\n",
        "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "            # Label the silhouette plots with their cluster numbers at the middle\n",
        "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "            # Compute the new y_lower for next plot\n",
        "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "        ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "        # The vertical line for average silhouette score of all the values\n",
        "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "        # 2nd Plot showing the actual clusters formed\n",
        "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "        ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                    c=colors, edgecolor='k')\n",
        "        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                      \"with n_clusters = %d\" % n_clusters),\n",
        "                    fontsize=14, fontweight='bold')\n",
        "        plt.show()\n",
        "        # # check variance using gaussian kde for each polygon\n",
        "        clusters = dict()\n",
        "        for i in range(len(colors)):\n",
        "          checksum = np.sum(colors[i])\n",
        "          if not checksum in clusters:\n",
        "            clusters[checksum] = []\n",
        "          clusters[checksum].append(X[i].tolist())\n",
        "        exterior_bounds = []\n",
        "        for cluster in clusters:\n",
        "          plt.cla()\n",
        "          plt.clf()\n",
        "          coordinates = np.array(clusters[cluster])\n",
        "          x = coordinates[:, 0]\n",
        "          y = coordinates[:, 1]\n",
        "          # Define the borders\n",
        "          values = np.vstack([x, y])\n",
        "          kernel = st.gaussian_kde(values)\n",
        "          # Define the borders\n",
        "          deltaX = (max(x) - min(x))/10\n",
        "          deltaY = (max(y) - min(y))/10\n",
        "          xmin = min(x) - deltaX\n",
        "          xmax = max(x) + deltaX\n",
        "          ymin = min(y) - deltaY\n",
        "          ymax = max(y) + deltaY\n",
        "          # Create meshgrid\n",
        "          xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j] \n",
        "\n",
        "          positions = np.vstack([xx.ravel(), yy.ravel()])\n",
        "          values = np.vstack([x, y])\n",
        "          f = np.reshape(kernel(positions).T, xx.shape)\n",
        "          fig = plt.figure(figsize=(8,8))\n",
        "          ax = fig.gca()\n",
        "          ax.set_xlim(xmin, xmax)\n",
        "          ax.set_ylim(ymin, ymax)\n",
        "          cfset = ax.contourf(xx, yy, f, cmap='coolwarm')\n",
        "          ax.imshow(np.rot90(f), cmap='coolwarm', extent=[xmin, xmax, ymin, ymax])\n",
        "          cset = ax.contour(xx, yy, f, colors='k')\n",
        "          ax.clabel(cset, inline=1, fontsize=10)\n",
        "          ax.set_xlabel('X')\n",
        "          ax.set_ylabel('Y')\n",
        "          plt.title('2D Gaussian Kernel density estimation')\n",
        "          plt.figure(figsize=(8,8))\n",
        "          for j in range(len(cset.allsegs)):\n",
        "            for ii, seg in enumerate(cset.allsegs[j]):\n",
        "              if j == 1 and ii == 0:\n",
        "                exterior_bounds.append(seg.tolist())\n",
        "              plt.plot(seg[:,0], seg[:,1], '.-', label=f'Cluster{j}, level{ii}')\n",
        "          plt.legend()\n",
        "          plt.show()\n",
        "          # plt.figure(figsize=(8,8))\n",
        "          # for j in range(len(cset.allsegs)):\n",
        "          #   for ii, seg in enumerate(cset.allsegs[j]):\n",
        "              \n",
        "        np.save(consensus_path + '/' + folder + '-' + file_name + '_machine_generated', exterior_bounds)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy6VzmW-RY27"
      },
      "source": [
        "## Test using meanshift"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7vUcnWdRbHa"
      },
      "source": [
        "def mean_shift_implementation():\n",
        "  x_shrink = 0.4\n",
        "  y_shrink = 0.4\n",
        "  # The generation will use all the available datasets that we have.\n",
        "  # It will traverse all the folders and all the files present in the clrs folder\n",
        "  # in order to obtain all the datasets.\n",
        "  folders = listdir(clrs_path)\n",
        "  for folder in folders:\n",
        "    all_files = listdir(clrs_path + '/' + folder)\n",
        "    for file_name in all_files:\n",
        "      file_name_without_csv = re.sub('\\.csv', '', file_name)\n",
        "      full_file_path = folder + '/' + file_name_without_csv\n",
        "\n",
        "      # This function fetches all games played on a specified dataset\n",
        "      polygons = random_plot_get_all_games(full_file_path)\n",
        "\n",
        "      # obtain the max and min coordinate to scale the value down to a 1x1 square\n",
        "      minX = 2147483647\n",
        "      minY = 2147483647\n",
        "      maxX = -2147483647\n",
        "      maxY = -2147483647\n",
        "      for polygon in polygons:\n",
        "        for coordinates in polygon:\n",
        "          minX = min(minX, coordinates[0])\n",
        "          maxX = max(maxX, coordinates[0])\n",
        "          minY = min(minY, coordinates[1])\n",
        "          maxY = max(maxY, coordinates[1])\n",
        "\n",
        "      # scale the coordinates between the range of [0,1]\n",
        "      scaled_polygons = []\n",
        "      for polygon in polygons:\n",
        "        temp_list = []\n",
        "        for coordinates in polygon:\n",
        "          x_val = (coordinates[0] - minX)/ (maxX - minX)\n",
        "          y_val = (coordinates[1] - minY)/ (maxY - minY)\n",
        "          temp_list.append(tuple([x_val, y_val]))\n",
        "        scaled_polygons.append(temp_list)\n",
        "\n",
        "      # If there is less than 30 games played, do not generate the consensus polygon as it is too small sample size\n",
        "      if len(polygons) < 30 :\n",
        "        continue\n",
        "\n",
        "      polygons = scaled_polygons\n",
        "      # totalArea = getTotalArea(polygons)\n",
        "      # print(\"This game has a total area of\" + str(totalArea))\n",
        "\n",
        "      # Get random points per polygon in order to later plot density plot\n",
        "      scatter_plot = []\n",
        "\n",
        "      # shrink down size of each polygon to get a better separation\n",
        "      for polygon in polygons:\n",
        "        xs, ys = zip(*polygon)\n",
        "        xs = np.array(xs)\n",
        "        ys = np.array(ys)\n",
        "        x_center = 0.5 * min(xs)  + 0.5 * max(xs)\n",
        "        y_center = 0.5 * min(ys)  + 0.5 * max(ys)\n",
        "        new_xs = [(i - x_center) * (1 - x_shrink) + x_center for i in xs]\n",
        "        new_ys = [(i - y_center) * (1 - y_shrink) + y_center for i in ys]\n",
        "        # create list of new coordinates\n",
        "        polygon = zip(new_xs, new_ys)\n",
        "        poly = Polygon(polygon)\n",
        "        polygonArea = PolyArea(xs,ys)\n",
        "        for i in range(int(100 * (polygonArea))):\n",
        "          point_in_poly = random_point_in_polygon(poly)\n",
        "          scatter_plot.append([point_in_poly.x, point_in_poly.y])\n",
        "        \n",
        "      # Store it in a numpy array so that it is easier to fetch all x coordinates and all y coordinates later\n",
        "      X = np.array(scatter_plot)\n",
        "      bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\n",
        "\n",
        "      ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
        "      ms.fit(X)\n",
        "      labels = ms.labels_\n",
        "      cluster_centers = ms.cluster_centers_\n",
        "\n",
        "      labels_unique = np.unique(labels)\n",
        "      n_clusters_ = len(labels_unique)\n",
        "\n",
        "      print(\"number of estimated clusters : %d\" % n_clusters_)\n",
        "\n",
        "      # #############################################################################\n",
        "      # Plot result\n",
        "\n",
        "      plt.figure(1)\n",
        "      plt.clf()\n",
        "\n",
        "      colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
        "      for k, col in zip(range(n_clusters_), colors):\n",
        "          my_members = labels == k\n",
        "          cluster_center = cluster_centers[k]\n",
        "          plt.plot(X[my_members, 0], X[my_members, 1], col + '.')\n",
        "          plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
        "                  markeredgecolor='k', markersize=14)\n",
        "      plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
        "      plt.show()\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sprZda-jYpLO"
      },
      "source": [
        "## DBSCAN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owxc7aBoYskI"
      },
      "source": [
        "def DBSCAN_implementation():\n",
        "  x_shrink = 0.4\n",
        "  y_shrink = 0.4\n",
        "  # The generation will use all the available datasets that we have.\n",
        "  # It will traverse all the folders and all the files present in the clrs folder\n",
        "  # in order to obtain all the datasets.\n",
        "  folders = listdir(clrs_path)\n",
        "  for folder in folders:\n",
        "    all_files = listdir(clrs_path + '/' + folder)\n",
        "    for file_name in all_files:\n",
        "      file_name_without_csv = re.sub('\\.csv', '', file_name)\n",
        "      full_file_path = folder + '/' + file_name_without_csv\n",
        "\n",
        "      # This function fetches all games played on a specified dataset\n",
        "      polygons = random_plot_get_all_games(full_file_path)\n",
        "\n",
        "      # obtain the max and min coordinate to scale the value down to a 1x1 square\n",
        "      minX = 2147483647\n",
        "      minY = 2147483647\n",
        "      maxX = -2147483647\n",
        "      maxY = -2147483647\n",
        "      for polygon in polygons:\n",
        "        for coordinates in polygon:\n",
        "          minX = min(minX, coordinates[0])\n",
        "          maxX = max(maxX, coordinates[0])\n",
        "          minY = min(minY, coordinates[1])\n",
        "          maxY = max(maxY, coordinates[1])\n",
        "\n",
        "      # scale the coordinates between the range of [0,1]\n",
        "      scaled_polygons = []\n",
        "      for polygon in polygons:\n",
        "        temp_list = []\n",
        "        for coordinates in polygon:\n",
        "          x_val = (coordinates[0] - minX)/ (maxX - minX)\n",
        "          y_val = (coordinates[1] - minY)/ (maxY - minY)\n",
        "          temp_list.append(tuple([x_val, y_val]))\n",
        "        scaled_polygons.append(temp_list)\n",
        "\n",
        "      # If there is less than 30 games played, do not generate the consensus polygon as it is too small sample size\n",
        "      if len(polygons) < 30 :\n",
        "        continue\n",
        "\n",
        "      polygons = scaled_polygons\n",
        "      # totalArea = getTotalArea(polygons)\n",
        "      # print(\"This game has a total area of\" + str(totalArea))\n",
        "\n",
        "      # Get random points per polygon in order to later plot density plot\n",
        "      scatter_plot = []\n",
        "\n",
        "      # shrink down size of each polygon to get a better separation\n",
        "      for polygon in polygons:\n",
        "        xs, ys = zip(*polygon)\n",
        "        xs = np.array(xs)\n",
        "        ys = np.array(ys)\n",
        "        x_center = 0.5 * min(xs)  + 0.5 * max(xs)\n",
        "        y_center = 0.5 * min(ys)  + 0.5 * max(ys)\n",
        "        new_xs = [(i - x_center) * (1 - x_shrink) + x_center for i in xs]\n",
        "        new_ys = [(i - y_center) * (1 - y_shrink) + y_center for i in ys]\n",
        "        # create list of new coordinates\n",
        "        polygon = zip(new_xs, new_ys)\n",
        "        poly = Polygon(polygon)\n",
        "        polygonArea = PolyArea(xs,ys)\n",
        "        for i in range(int(100 * (polygonArea))):\n",
        "          point_in_poly = random_point_in_polygon(poly)\n",
        "          scatter_plot.append([point_in_poly.x, point_in_poly.y])\n",
        "        \n",
        "      # Store it in a numpy array so that it is easier to fetch all x coordinates and all y coordinates later\n",
        "      X = np.array(scatter_plot)\n",
        "      # Compute DBSCAN\n",
        "      db = DBSCAN(eps=0.2, min_samples=200).fit(X)\n",
        "      core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
        "      core_samples_mask[db.core_sample_indices_] = True\n",
        "      labels = db.labels_\n",
        "\n",
        "      # Number of clusters in labels, ignoring noise if present.\n",
        "      n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "      n_noise_ = list(labels).count(-1)\n",
        "\n",
        "      # #############################################################################\n",
        "      # Plot result\n",
        "      import matplotlib.pyplot as plt\n",
        "\n",
        "      # Black removed and is used for noise instead.\n",
        "      unique_labels = set(labels)\n",
        "      colors = [plt.cm.Spectral(each)\n",
        "                for each in np.linspace(0, 1, len(unique_labels))]\n",
        "      for k, col in zip(unique_labels, colors):\n",
        "          if k == -1:\n",
        "              # Black used for noise.\n",
        "              col = [0, 0, 0, 1]\n",
        "\n",
        "          class_member_mask = (labels == k)\n",
        "\n",
        "          xy = X[class_member_mask & core_samples_mask]\n",
        "          plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "                  markeredgecolor='k', markersize=14)\n",
        "\n",
        "          xy = X[class_member_mask & ~core_samples_mask]\n",
        "          plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "                  markeredgecolor='k', markersize=6)\n",
        "\n",
        "      plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
        "      plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3TcvRncku0Y"
      },
      "source": [
        "# DBSCAN_implementation()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}